# ðŸ¦ Fed Data Scraper

**Fed Data Scraper** is a pipeline for extracting and structuring data from Federal Reserve Y-6 filings. It uses OCR, LLMs, and markdown parsing to turn unstructured PDFs into usable CSVs.

---

##  Features

- Reads documents from AWS S3
- OCR via Mistral
- LLM data parsing using Gemini
- Markdown â†’ Structured JSON â†’ CSV
- Supports insider and shareholder data
- Tracks success/failure status per file

---

## âš™ï¸ Configuration via `.env`

All sensitive credentials are stored in a `.env` file for secure and flexible usage.

###  Sample `.env` Format

```env

COOKIES='[
    {"name": "BrowserGUID", "value": "...", "domain": ".capitaliq.spglobal.com"},
    {"name": "CIQP", "value": "true", "domain": ".capitaliq.spglobal.com"},
    {"name": "EKOU", "value": "...", "domain": ".capitaliq.spglobal.com"},
    ...
    {"name": "SNL_OAUTH_TOKEN1", "value": "...", "domain": ".capitaliq.spglobal.com"}
]'
MISTRAL_API_KEY='your_mistral_api_key_here'
GENAI_API_KEY='your_google_gemini_api_key_here'
```

> ðŸ’¡ You can retrieve \`COOKIES\` by exporting from your browser (e.g., using Chrome DevTools or browser extensions like "EditThisCookie").

---

##  File Overview

### `scraper.py`
- Downloads documents and extracts markdown via OCR.
- Sends PDF URLs to Mistral API.
- Stores markdown to S3 and `json/`.
- Scrapes the pages in the `pages_to_scrape` variable

### `read_json.py`
- Reads markdown from `json/`.
- Uses Gemini API to extract structured data.
- Outputs CSVs for insiders and shareholders.

### `read_pdfs.py`
- Alternative flow: direct PDF parsing using `pdfplumber`.
- Bypasses OCR for debugging or fallback.

---

## ðŸ—‚ Folder Structure


```
.
â”œâ”€â”€ Gemini
â”‚   â”œâ”€â”€ csv_testing
â”‚   â”‚   â”œâ”€â”€ insiders
â”‚   â”‚   â”œâ”€â”€ insiders_second
â”‚   â”‚   â”œâ”€â”€ securities
â”‚   â”‚   â””â”€â”€ securities_second
â”‚   â”œâ”€â”€ read_json.py
â”‚   â””â”€â”€ read_json_second_prompt.py
â”œâ”€â”€ Mistral 
â”‚   â”œâ”€â”€ read_CapIQ_pdfs.py
â”‚   â”œâ”€â”€ read_cleveland_pdfs.py
â”‚   â”œâ”€â”€ read_dallas_pdfs.py
â”‚   â”œâ”€â”€ read_minneapolis_pdfs.py
â”‚   â””â”€â”€ read_richmond_pdfs.py
â”œâ”€â”€ README.md
â”œâ”€â”€ Scraper
â”‚   â”œâ”€â”€ collect_failed_pages_CapIQ.py
â”‚   â”œâ”€â”€ scraper_CapIQ.py
â”‚   â”œâ”€â”€ scraper_cleveland.py
â”‚   â”œâ”€â”€ scraper_dallas.py
â”‚   â”œâ”€â”€ scraper_minneapolis.py
â”‚   â””â”€â”€ scraper_richmond.py
â”œâ”€â”€ cookies.json ## make sure to create this file when pulling code. Paste your cookies exactly as exported from cookies extention
â”œâ”€â”€ cookies.py ## After pasting your cookies in cookies.json, run this script and copy the terminal output directly into your .env file. 
â”œâ”€â”€ documents         # the list of PDF urls for these districts live here
â”‚   â”œâ”€â”€ Dallas_JSON.json
â”‚   â”œâ”€â”€ Minneapolis_JSON.json
â”‚   â””â”€â”€ Richmond_JSON.json
â”œâ”€â”€ helper 
â”‚   â”œâ”€â”€ count_processed_failed.py
â”‚   â”œâ”€â”€ count_scraped_failed.py
â”‚   â”œâ”€â”€ upload_processed_mistral.py
â”‚   â””â”€â”€ upload_scraped_to_S3.py
â”œâ”€â”€ notebook
â”‚   â”œâ”€â”€ combine.py
â”‚   â””â”€â”€ data_figures.ipynb
â”œâ”€â”€ processed_mistral
â”œâ”€â”€ requirements.txt
â””â”€â”€ scraped_files
â””â”€â”€ .env ## your credentials will live here, including your formatted cookies, gemini, and mistral key. 

```

---

## â¬‡ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/fed-data-scraper.git
cd fed-data-scraper
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

> Required packages include: `boto3`, `pdfplumber`, `pandas`, `google-generativeai`, `mistralai`, `python-dotenv`, etc.

### 3. Configure `.env`

Add your credentials to a file called `.env` in the root directory, using the format above.

---

##  Usage

### OCR Pipeline: PDF to markdown

```bash
python read_CapIQ_pdfs.py #CAPIQ data
```

### LLM Parsing: markdown to CSV

```bash
python read_json.py
```

### (Optional) Local PDF Parsing

```bash
python read_pdfs.py
```

---

## ðŸ“ Output

- CSVs saved locally to `/csv/` and uploaded to S3 (if configured).
- Logs for failed and successful file parses.

---

## ðŸ›‘ Notes

- Ensure your S3 bucket follows the expected structure: `/documents/`, `/json/`, and `/csv/`.
- OCR and LLM performance varies based on PDF quality.
- Requires valid Mistral and Google API keys.
---

## ðŸ” AWS Configuration

This project requires access to AWS S3 for uploading/downloading documents and results.

### ðŸŸ¡ AWS CLI Setup

Ensure you have the AWS CLI installed and configured:

```bash
aws configure
```

You'll be prompted to enter:

- AWS Access Key ID
- AWS Secret Access Key
- Default region name
- Output format (optional)

> ðŸ”‘ The credentials are stored in `~/.aws/credentials` and are used by `boto3` to interact with S3.

Make sure your IAM user has appropriate S3 permissions for the required buckets:

```json
{
  "Effect": "Allow",
  "Action": [
    "s3:GetObject",
    "s3:PutObject",
    "s3:ListBucket"
  ],
  "Resource": [
    "arn:aws:s3:::your-bucket-name",
    "arn:aws:s3:::your-bucket-name/*"
  ]
}
```
